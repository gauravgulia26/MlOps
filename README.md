# ğŸ› ï¸ ETL Pipeline with Airflow, API Handling & Dockerized PostgreSQL ğŸš€

This project demonstrates an **ETL pipeline** using **Apache Airflow**, which fetches data from an **API**, transforms the data, and loads it into a **PostgreSQL database** running inside a **Docker container**. Additionally, the pipeline includes **API health check handling** to ensure smooth operation.

## ğŸ“¦ Requirements

Before running the pipeline, make sure you have the following tools installed:
- **Docker** ğŸ³
- **Apache Airflow** âœˆï¸
- **PostgreSQL** (within Docker) ğŸ‡
- Python Libraries:
  - `requests` ğŸŒ
  - `psycopg2` ğŸ“š
  - `airflow` ğŸ§‘â€ğŸ’»

## ğŸš€ Features

- **ETL Pipeline**:
  - Extracts data from an external **API** ğŸ–¥ï¸.
  - Transforms the data as per your business logic ğŸ”„.
  - Loads the data into a **PostgreSQL** database running inside a **Docker container** ğŸ—„ï¸.

- **API Health Check** ğŸ©º:
  - Implements an API handling feature to check if the API is **up or down**.
  - Sends a notification if the API is not reachable ğŸš«.

- **Dockerized PostgreSQL** ğŸ·:
  - PostgreSQL database is containerized using Docker for easy deployment and scalability.
